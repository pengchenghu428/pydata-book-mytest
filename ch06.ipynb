{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.1文本格式数据的读写\n",
    "由于现实世界的数据非常混乱，随着事件推移，一些数据夹杂自函数（尤其是read_csv）的可选参数变得非常复杂。pandas在线文档有大量的示例展示这些参数是如何工作的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   a   b   c   d message\n",
      "0  1   2   3   4   hello\n",
      "1  5   6   7   8   world\n",
      "2  9  10  11  12     foo\n",
      "   a   b   c   d message\n",
      "0  1   2   3   4   hello\n",
      "1  5   6   7   8   world\n",
      "2  9  10  11  12     foo\n",
      "   0   1   2   3      4\n",
      "0  1   2   3   4  hello\n",
      "1  5   6   7   8  world\n",
      "2  9  10  11  12    foo\n",
      "   a   b   c   d message\n",
      "0  1   2   3   4   hello\n",
      "1  5   6   7   8   world\n",
      "2  9  10  11  12     foo\n",
      "         a   b   c   d\n",
      "message               \n",
      "hello    1   2   3   4\n",
      "world    5   6   7   8\n",
      "foo      9  10  11  12\n",
      "           value1  value2\n",
      "key1 key2                \n",
      "one  a          1       2\n",
      "     b          3       4\n",
      "     c          5       6\n",
      "     d          7       8\n",
      "two  a          9      10\n",
      "     b         11      12\n",
      "     c         13      14\n",
      "     d         15      16\n"
     ]
    }
   ],
   "source": [
    "%pwd\n",
    "df1 = pd.read_csv('examples/ex1.csv')\n",
    "print(df1)\n",
    "\n",
    "# 使用read_table，并指定分割符\n",
    "df2 = pd.read_table('examples/ex1.csv', sep=',')\n",
    "print(df2)\n",
    "\n",
    "# 有的文件不不包含表头行，可以允许pandas自动分配默认列名，也可以自己指定列名\n",
    "df3 = pd.read_csv('examples/ex2.csv', header=None)\n",
    "print(df3)\n",
    "df4 = pd.read_csv('examples/ex2.csv', names=['a', 'b', 'c', 'd', 'message'])\n",
    "print(df4)\n",
    "\n",
    "# 指定某一位置的列为索引\n",
    "names = ['a', 'b', 'c', 'd', 'message']\n",
    "df5 = pd.read_csv('examples/ex2.csv', names=names, index_col='message')\n",
    "print(df5)\n",
    "\n",
    "# 当你需要从多个列中形成一个分层所有，需要传入一个包含序列号或列名的列表\n",
    "parsed = pd.read_csv('examples/csv_mindex.csv', index_col=['key1','key2'])\n",
    "print(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            A         B         C\n",
      "aaa -0.264438 -1.026059 -0.619500\n",
      "bbb  0.927272  0.302904 -0.032399\n",
      "ccc -0.264273 -0.386314 -0.217601\n",
      "ddd -0.871858 -0.348382  1.100491\n",
      "   a   b   c   d message\n",
      "0  1   2   3   4   hello\n",
      "1  5   6   7   8   world\n",
      "2  9  10  11  12     foo\n",
      "  something  a   b     c   d message\n",
      "0       one  1   2   3.0   4     NaN\n",
      "1       two  5   6   NaN   8   world\n",
      "2     three  9  10  11.0  12     foo\n",
      "  something  a   b     c   d message\n",
      "0       one  1   2   3.0   4     NaN\n",
      "1       two  5   6   NaN   8   world\n",
      "2     three  9  10  11.0  12     foo\n",
      "  something  a   b     c   d message\n",
      "0       one  1   2   3.0   4     NaN\n",
      "1       NaN  5   6   NaN   8   world\n",
      "2     three  9  10  11.0  12     NaN\n"
     ]
    }
   ],
   "source": [
    "# 当字段是以不同数量的空格分开时，可以通过向read_table传入一个正则表达式作为分隔符\n",
    "result = pd.read_table('examples/ex3.txt', sep='\\s+')\n",
    "print(result)\n",
    "\n",
    "# 可以使用skiprows 来跳过某些行\n",
    "print(pd.read_csv('examples/ex4.csv', skiprows=[0, 2, 3]))\n",
    "\n",
    "# 缺失值处理\n",
    "# 默认情况下，pandas使用常见的标识，例如NA和NULL\n",
    "print(pd.read_csv('examples/ex5.csv'))\n",
    "# na_values选项可以传入一个列表或者一组字符串来处理缺失值，即需要用NA替换的值序列\n",
    "print(pd.read_csv('examples/ex5.csv', na_values=['NULL']))\n",
    "# 在字典中，每列可以指定不同的缺失值标识\n",
    "sentinels = {'message': ['foo', 'NA'], 'something': ['two']}\n",
    "print(pd.read_csv('examples/ex5.csv',na_values=sentinels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.1.1 分块读入文本文件\n",
    "当处理大型文件或找出正确的参数集来正确处理大文件时，可能需要读入文件的一个小片段或者按小块遍历文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           one       two     three      four key\n",
      "0     0.467976 -0.038649 -0.295344 -1.824726   L\n",
      "1    -0.358893  1.404453  0.704965 -0.200638   B\n",
      "2    -0.501840  0.659254 -0.421691 -0.057688   G\n",
      "3     0.204886  1.074134  1.388361 -0.982404   R\n",
      "4     0.354628 -0.133116  0.283763 -0.837063   Q\n",
      "...        ...       ...       ...       ...  ..\n",
      "9995  2.311896 -0.417070 -1.409599 -0.515821   L\n",
      "9996 -0.479893 -0.650419  0.745152 -0.646038   E\n",
      "9997  0.523331  0.787112  0.486066  1.093156   K\n",
      "9998 -0.362559  0.598894 -1.843201  0.887292   G\n",
      "9999 -0.096376 -1.012999 -0.657431 -0.573315   0\n",
      "\n",
      "[10000 rows x 5 columns]\n",
      "        one       two     three      four key\n",
      "0  0.467976 -0.038649 -0.295344 -1.824726   L\n",
      "1 -0.358893  1.404453  0.704965 -0.200638   B\n",
      "2 -0.501840  0.659254 -0.421691 -0.057688   G\n",
      "3  0.204886  1.074134  1.388361 -0.982404   R\n",
      "4  0.354628 -0.133116  0.283763 -0.837063   Q\n",
      "<class 'pandas.io.parsers.TextFileReader'>\n",
      "E    368.0\n",
      "X    364.0\n",
      "L    346.0\n",
      "O    343.0\n",
      "Q    340.0\n",
      "M    338.0\n",
      "J    337.0\n",
      "F    335.0\n",
      "K    334.0\n",
      "H    330.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 在尝试大文件之前，我们可以先对pandas的显示设置进行调整，使之更为紧凑\n",
    "pd.options.display.max_rows = 10\n",
    "result = pd.read_csv('examples/ex6.csv')\n",
    "print(result)\n",
    "\n",
    "# 读取一小部分（避免读取整个文件），可以知名nrows\n",
    "print(pd.read_csv('examples/ex6.csv', nrows=5))\n",
    "\n",
    "# 为了分块读入文件，可以指定chunksize作为每一块的行数\n",
    "chunksize = pd.read_csv('examples/ex6.csv', chunksize=1000)\n",
    "print(type(chunksize))\n",
    "# read_csv返回的TextParser对象允许你根据chunksize遍历文件。\n",
    "# 例如，便利ex6.csv，并对‘key’列聚合获得计数值\n",
    "tot = pd.Series([])\n",
    "for piece in chunksize:\n",
    "    tot = tot.add(piece['key'].value_counts(), fill_value=0)\n",
    "    \n",
    "tot = tot.sort_values(ascending=False)\n",
    "print(tot[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.1.2 将数据写入文本格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  something  a   b     c   d message\n",
      "0       one  1   2   3.0   4     NaN\n",
      "1       two  5   6   NaN   8   world\n",
      "2     three  9  10  11.0  12     foo\n",
      "|something|a|b|c|d|message\n",
      "0|one|1|2|3.0|4|\n",
      "1|two|5|6||8|world\n",
      "2|three|9|10|11.0|12|foo\n",
      ",something,a,b,c,d,message\n",
      "0,one,1,2,3.0,4,NaN\n",
      "1,two,5,6,NaN,8,world\n",
      "2,three,9,10,11.0,12,foo\n",
      "one,1,2,3.0,4,\n",
      "two,5,6,,8,world\n",
      "three,9,10,11.0,12,foo\n",
      "a,b,c\n",
      "1,2,3.0\n",
      "5,6,\n",
      "9,10,11.0\n",
      "2018-09-27,0\n",
      "2018-09-28,1\n",
      "2018-09-29,2\n",
      "2018-09-30,3\n",
      "2018-10-01,4\n",
      "2018-10-02,5\n",
      "2018-10-03,6\n"
     ]
    }
   ],
   "source": [
    "# 读取CSV文件\n",
    "data = pd.read_csv('examples/ex5.csv')\n",
    "print(data)\n",
    "\n",
    "# 使用DataFrame的to_csv方法，我们可以将数据到处为逗号分隔的文件\n",
    "data.to_csv('examples/out.csv')\n",
    "\n",
    "# 使用其他的分隔符（写入到sys.stdout）\n",
    "import sys\n",
    "data.to_csv(sys.stdout, sep='|')\n",
    "\n",
    "# 缺失值在输出时以空白字符串出现，可以用其他标识值对缺失值进行标注\n",
    "data.to_csv(sys.stdout, na_rep='NaN')\n",
    "\n",
    "# 如果没有其它选项被指定的话，行和列的标签都会被写入，不过二者也都可以禁止\n",
    "data.to_csv(sys.stdout, index=False, header=False)\n",
    "\n",
    "# 仅仅写入列的自己，并且按照选择的顺序写入\n",
    "data.to_csv(sys.stdout, index=False, columns=['a', 'b', 'c'])\n",
    "\n",
    "# Series也有to_csv方法\n",
    "dates = pd.date_range('27/9/2018', periods=7)\n",
    "ts = pd.Series(np.arange(7), index=dates)\n",
    "ts.to_csv(sys.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.1.3 使用分割格式\n",
    "绝大多数的表型数据都可以使用函数pandas.readd_table从硬盘中读取。然而，在某些情况下，一些手动操作时必不可少的。接收一个带有一行或多行错误的文件并不少见，read_table也无法解决这种情况。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': ('1', '1'), 'b': ('2', '2'), 'c': ('3', '3')}\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# 将文件读取为行的列表\n",
    "with open('examples/ex7.csv') as f:\n",
    "    lines = list(csv.reader(f))\n",
    "\n",
    "# 将数据拆分为列名行和数据行\n",
    "header, values = lines[0], lines[1:]\n",
    "\n",
    "# 使用字典推到是和表达式zip(*values)生成一个包含数据列的字典，字典中行专制成列\n",
    "data_dict = {h: v for h, v in zip(header, zip(*values))}\n",
    "print(data_dict)\n",
    "\n",
    "# CSV文件有多种不同分格。如需根据不同的分隔符、字符串引用约定或行中止符定义一种新的格式时，我们可以使用CSV_Dialect定义一个简单的子类。\n",
    "class my_dialect(csv.Dialect):\n",
    "    lineterminator = '\\n' # 行终止符，默认'\\r\\n'，读取器会忽略行中止符兵识别跨平台行中止符\n",
    "    delimiter = ';' # 一个用于分割字段的字符，默认是','\n",
    "    quotechar = '\"' # 用在含有特殊字符字段中的引号，默认是'\"'\n",
    "    quoting = csv.QUOTE_MINIMAL # 引用管理。选项包括csv.QUOTE_ALL(引用所有字段)，csv.QUOTE_MINIMAL(只使用特殊符号，如分隔符)，csv.QUOTE_NONNUMERIC和csv.QUOTE_NONE(不引用)。细节参考Python文档，默认是QUOTE_MINIMAL\n",
    "    skipinitialspace = False # 忽略每个分隔符后的空白，默认是False\n",
    "    doublequote = False # 如何处理字段内部引号。如果是True，则是双引号（完整细节和行为请参考在线文档）\n",
    "    # escapechar = None # 当引用设置为csv.QUOTE_NONE时用于转义分隔符的字符串，默认禁用 \n",
    "\n",
    "with open('examples/mydata.csv', 'w') as f:\n",
    "    writer = csv.writer(f, dialect=my_dialect)\n",
    "    writer.writerow(('one', 'two', 'three'))\n",
    "    writer.writerow(('1', '2', '3'))\n",
    "    writer.writerow(('4', '5', '6'))\n",
    "    writer.writerow(('7', '8', '9'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.1.4 JSON数据\n",
    "JSON(JavaScript Object Notation)已经成为Web浏览器和其他应用通过HTTP请求发送数据的标准格式。它是一种比CSV等表格文本形式更为自由的数据形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  b  c\n",
       "0  1  2  3\n",
       "1  4  5  6\n",
       "2  7  8  9"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_json('examples/example.json')\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Bank Name             City  ST   CERT  \\\n",
      "0                   Allied Bank         Mulberry  AR     91   \n",
      "1  The Woodbury Banking Company         Woodbury  GA  11297   \n",
      "2        First CornerStone Bank  King of Prussia  PA  35312   \n",
      "3            Trust Company Bank          Memphis  TN   9956   \n",
      "4    North Milwaukee State Bank        Milwaukee  WI  20364   \n",
      "\n",
      "                 Acquiring Institution        Closing Date       Updated Date  \n",
      "0                         Today's Bank  September 23, 2016  November 17, 2016  \n",
      "1                          United Bank     August 19, 2016  November 17, 2016  \n",
      "2  First-Citizens Bank & Trust Company         May 6, 2016  September 6, 2016  \n",
      "3           The Bank of Fayette County      April 29, 2016  September 6, 2016  \n",
      "4  First-Citizens Bank & Trust Company      March 11, 2016      June 16, 2016  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23    31\n",
       "30    30\n",
       "19    30\n",
       "20    27\n",
       "17    23\n",
       "16    22\n",
       "4     21\n",
       "18    21\n",
       "7     20\n",
       "15    20\n",
       "2     20\n",
       "14    20\n",
       "24    19\n",
       "28    18\n",
       "27    18\n",
       "22    18\n",
       "11    18\n",
       "21    17\n",
       "29    16\n",
       "26    15\n",
       "5     15\n",
       "13    14\n",
       "10    14\n",
       "8     14\n",
       "6     14\n",
       "25    13\n",
       "12    10\n",
       "9     10\n",
       "31     8\n",
       "1      8\n",
       "3      3\n",
       "Name: Closing Date, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pandas.read_html 函数有很多选项，但是默认情况下，它会搜索并尝试解析所有包含在<table>标签中的表格型数据，返回的结果是DataFrame对象的列表\n",
    "tables = pd.read_html('examples/fdic_failed_bank_list.html')\n",
    "failures = tables[0]\n",
    "print(failures.head())\n",
    "\n",
    "# 数据清洗和分析工作，比如计算每年银行倒闭的数量\n",
    "close_timestamps = pd.to_datetime(failures['Closing Date'])\n",
    "close_timestamps.dt.year.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.1.5.1 使用lxml.objectify 解析XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            AGENCY_NAME            CATEGORY  \\\n",
      "0  Metro-North Railroad  Service Indicators   \n",
      "1  Metro-North Railroad  Service Indicators   \n",
      "2  Metro-North Railroad  Service Indicators   \n",
      "3  Metro-North Railroad  Service Indicators   \n",
      "4  Metro-North Railroad  Service Indicators   \n",
      "\n",
      "                                         DESCRIPTION FREQUENCY  \\\n",
      "0  Percent of commuter trains that arrive at thei...         M   \n",
      "1  Percent of commuter trains that arrive at thei...         M   \n",
      "2  Percent of commuter trains that arrive at thei...         M   \n",
      "3  Percent of commuter trains that arrive at thei...         M   \n",
      "4  Percent of commuter trains that arrive at thei...         M   \n",
      "\n",
      "                         INDICATOR_NAME INDICATOR_UNIT MONTHLY_ACTUAL  \\\n",
      "0  On-Time Performance (West of Hudson)              %           96.9   \n",
      "1  On-Time Performance (West of Hudson)              %             95   \n",
      "2  On-Time Performance (West of Hudson)              %           96.9   \n",
      "3  On-Time Performance (West of Hudson)              %           98.3   \n",
      "4  On-Time Performance (West of Hudson)              %           95.8   \n",
      "\n",
      "  MONTHLY_TARGET  PERIOD_MONTH  PERIOD_YEAR YTD_ACTUAL YTD_TARGET  \n",
      "0             95             1         2008       96.9         95  \n",
      "1             95             2         2008         96         95  \n",
      "2             95             3         2008       96.3         95  \n",
      "3             95             4         2008       96.8         95  \n",
      "4             95             5         2008       96.6         95  \n",
      "Google\n",
      "http://www.google.hk\n",
      "Google\n"
     ]
    }
   ],
   "source": [
    "from lxml import objectify\n",
    "\n",
    "path = 'datasets/mta_perf/Performance_MNR.xml'\n",
    "parsed = objectify.parse(open(path))\n",
    "root = parsed.getroot()\n",
    "\n",
    "data = []\n",
    "skip_fields = ['INDICATOR_SEQ', 'PARENT_SEQ', 'DESIRED_CHANGE', 'DECIMAL_PLACES']\n",
    "\n",
    "for elt in root.INDICATOR: # 遍历root下的所有INDICATOR\n",
    "    el_data = {}\n",
    "    for child in elt.getchildren(): # 遍历INDICATOR下所有的子节点\n",
    "        if child.tag in skip_fields:\n",
    "            continue\n",
    "        el_data[child.tag] = child.pyval\n",
    "    data.append(el_data)\n",
    "\n",
    "perf = pd.DataFrame(data)\n",
    "print(perf.head())\n",
    "\n",
    "# XML数据可以更复杂，每个班标签也可以包含元数据。考虑一个HTML连接标签，也可以是有效的XML\n",
    "from io import StringIO\n",
    "tag = '<a href=\"http://www.google.hk\">Google</a>'\n",
    "root = objectify.parse(StringIO(tag)).getroot()\n",
    "\n",
    "print(root.get('href'))\n",
    "print(root.text)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "6.2 二进制格式\n",
    "使用Python 内建的pickle 序列化模块进行二进制格式操作是存储数据（也称为序列化）最高效、最方便的方式之一。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   a   b   c   d message\n",
      "0  1   2   3   4   hello\n",
      "1  5   6   7   8   world\n",
      "2  9  10  11  12     foo\n",
      "   a   b   c   d message\n",
      "0  1   2   3   4   hello\n",
      "1  5   6   7   8   world\n",
      "2  9  10  11  12     foo\n"
     ]
    }
   ],
   "source": [
    "frame = pd.read_csv('examples/ex1.csv')\n",
    "print(frame)\n",
    "# pandas对象拥有一个to_pickle 方法可以将数据以pickle 格式写入硬盘\n",
    "frame.to_pickle('examples/frame_pickle')\n",
    "\n",
    "# 可以直接使用内建的pickle读取文件中的\"pickle化\"对象，或更方便地 使用pandas.read_pickle\n",
    "print(pd.read_pickle('examples/frame_pickle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.2.1 使用HDF5格式\n",
    "HDF5是一个备受好评的文件格式，用于存储大量的科学数组数据。 HDF5适合处理不适合在内存中存储的大型数据，可以使你高校读写大型数据组的一小块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "HDFStore requires PyTables, \"No module named 'tables'\" problem importing",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32md:\\programfiles\\python\\python37\\lib\\site-packages\\pandas\\io\\pytables.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path, mode, complevel, complib, fletcher32, **kwargs)\u001b[0m\n\u001b[0;32m    468\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 469\u001b[1;33m             \u001b[1;32mimport\u001b[0m \u001b[0mtables\u001b[0m  \u001b[1;31m# noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    470\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pragma: no cover\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tables'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-1dd2ca3d5550>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# pandas 提供HDFStore 类像字典一样工作并处理低级别的细节\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mstore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHDFStore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'examples/mydata.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mstore\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'obj1'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mstore\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'obj1_col'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programfiles\\python\\python37\\lib\\site-packages\\pandas\\io\\pytables.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path, mode, complevel, complib, fletcher32, **kwargs)\u001b[0m\n\u001b[0;32m    470\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pragma: no cover\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    471\u001b[0m             raise ImportError('HDFStore requires PyTables, \"{ex}\" problem '\n\u001b[1;32m--> 472\u001b[1;33m                               'importing'.format(ex=str(ex)))\n\u001b[0m\u001b[0;32m    473\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcomplib\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcomplib\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtables\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall_complibs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: HDFStore requires PyTables, \"No module named 'tables'\" problem importing"
     ]
    }
   ],
   "source": [
    "# pandas 提供HDFStore 类像字典一样工作并处理低级别的细节\n",
    "# 未能成功安装工具包库\n",
    "frame = pd.DataFrame({'a': np.random.randn(100)})\n",
    "store = pd.HDFStore('examples/mydata.h5')\n",
    "store['obj1'] = frame\n",
    "store['obj1_col'] = frame['a']\n",
    "\n",
    "print(store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果处理存储在远程服务器上的数据时，不如Amazon S3或HDFS，使用其他专门为分布式存储而设计的二进制格式更为合适，比如Apache Parquet。Parquet和其他类似的存储格式仍然在发展中。\n",
    "如果是在本地处理大量数据，推荐尝试PyTables和h5py，看看他们是否符合你的需求。由于很多数据分析的困难在于I/O密集（而不是CPU密集），使用像HDF5这样的工具可以大大加速你的应用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.2.2 读取Microdoft Excel文件\n",
    "pandas 也支持通过ExcelFile 类或pandas.read_excel函数来读取存储在Excel文件中的表格行数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   a   b   c   d message\n",
      "0  1   2   3   4   hello\n",
      "1  5   6   7   8   world\n",
      "2  9  10  11  12     foo\n",
      "   a   b   c   d message\n",
      "0  1   2   3   4   hello\n",
      "1  5   6   7   8   world\n",
      "2  9  10  11  12     foo\n"
     ]
    }
   ],
   "source": [
    "# 使用ExcelFile\n",
    "xlsx = pd.ExcelFile('examples/ex1.xlsx')\n",
    "\n",
    "# 存储在表中的数据可以通过pandas.read_excel 读取到DataFrame\n",
    "ex1 = pd.read_excel(xlsx, 'Sheet1')\n",
    "print(ex1)\n",
    "\n",
    "# 如果读取的是含有多个表的文件，生成ExcelFile更快，但你也可以更简洁地将文件名传入pandas.read_excel\n",
    "frame = pd.read_excel('examples/ex1.xlsx', 'Sheet1')\n",
    "print(frame)\n",
    "\n",
    "# 将pandas数据写入到Excel表格中\n",
    "writer = pd.ExcelWriter('examples/ex2.xlsx')\n",
    "frame.to_excel(writer, 'Sheet1')\n",
    "writer.save()\n",
    "\n",
    "# 也可以直接将文件路径传给to_excel，避免直接调用ExcelWriter\n",
    "frame.to_excel('examples/ex2.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.3 与Web API交互\n",
    "很多网站都有公开API，通过JSON 或其他数据格式提供数据服务。有多种方式可以利用Python 来访问API；推荐的简单易用方式是使用request包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fix quantile docstring\n",
      "    number                                              title  \\\n",
      "0    22906                             fix quantile docstring   \n",
      "1    22905  pandas/tests/indexes/datetimes/test_astype.py ...   \n",
      "2    22904  DOC GH22897 Fix docstring of join in pandas/co...   \n",
      "3    22903              lib.is_scalar misses PEP 3141 numbers   \n",
      "4    22902  DOC: #22899, Fixed docstring of itertuples in ...   \n",
      "5    22901    CLN GH22873 Replace base excepts in pandas/core   \n",
      "6    22900   DOC/STYLE: Fix \"F821 undefined name 'pd'\" errors   \n",
      "7    22899  DOC: Fix the docstring of itertuples in pandas...   \n",
      "8    22898  DOC: Fix the docstring of quantile in pandas/c...   \n",
      "9    22897  DOC: Fix the docstring of join in pandas/core/...   \n",
      "10   22896  DOC: Fix the docstring of to_stata in pandas/c...   \n",
      "11   22895  DOC: Fix the docstring of _set_axis_name in pa...   \n",
      "12   22894  DOC: Fix the docstring of resample in pandas/c...   \n",
      "13   22893  DOC: Fix the docstring of groupby in pandas/co...   \n",
      "14   22892  DOC: Fix the docstring of xs in pandas/core/ge...   \n",
      "15   22890          DOC: updated the docstring of Series.dot    \n",
      "16   22889    BLD: minor break ci/requirements-optional-*.txt   \n",
      "17   22888  TST: coverage for skipped tests in io/formats/...   \n",
      "18   22887  TypeError with to_html(sparsify=False) and max...   \n",
      "19   22885  STYLE: Fix lint error \"F811 redefinition of un...   \n",
      "20   22882  Remove bare exceptions from 'pandas/tests' to ...   \n",
      "21   22881                     Docs for ParserError are wrong   \n",
      "22   22880         WIP: Use align_method in comp_method_FRAME   \n",
      "23   22879                        CLN: remove Index._to_embed   \n",
      "24   22877  Replace bare excepts by explicit excepts in pa...   \n",
      "25   22876  date_range method doesn't work when start and ...   \n",
      "26   22875  Replace bare excepts by explicit excepts in pa...   \n",
      "27   22874  Replace bare excepts by explicit excepts in pa...   \n",
      "28   22873  Replace bare excepts by explicit excepts in pa...   \n",
      "29   22872  Replace bare excepts by explicit excepts in pa...   \n",
      "\n",
      "                                               labels state  \n",
      "0                                                  []  open  \n",
      "1                                                  []  open  \n",
      "2                                                  []  open  \n",
      "3                                                  []  open  \n",
      "4                                                  []  open  \n",
      "5                                                  []  open  \n",
      "6   [{'id': 134699, 'node_id': 'MDU6TGFiZWwxMzQ2OT...  open  \n",
      "7   [{'id': 134699, 'node_id': 'MDU6TGFiZWwxMzQ2OT...  open  \n",
      "8   [{'id': 134699, 'node_id': 'MDU6TGFiZWwxMzQ2OT...  open  \n",
      "9   [{'id': 134699, 'node_id': 'MDU6TGFiZWwxMzQ2OT...  open  \n",
      "10  [{'id': 134699, 'node_id': 'MDU6TGFiZWwxMzQ2OT...  open  \n",
      "11  [{'id': 134699, 'node_id': 'MDU6TGFiZWwxMzQ2OT...  open  \n",
      "12  [{'id': 134699, 'node_id': 'MDU6TGFiZWwxMzQ2OT...  open  \n",
      "13  [{'id': 134699, 'node_id': 'MDU6TGFiZWwxMzQ2OT...  open  \n",
      "14  [{'id': 134699, 'node_id': 'MDU6TGFiZWwxMzQ2OT...  open  \n",
      "15  [{'id': 134699, 'node_id': 'MDU6TGFiZWwxMzQ2OT...  open  \n",
      "16  [{'id': 129350, 'node_id': 'MDU6TGFiZWwxMjkzNT...  open  \n",
      "17                                                 []  open  \n",
      "18                                                 []  open  \n",
      "19  [{'id': 48070600, 'node_id': 'MDU6TGFiZWw0ODA3...  open  \n",
      "20  [{'id': 211029535, 'node_id': 'MDU6TGFiZWwyMTE...  open  \n",
      "21  [{'id': 134699, 'node_id': 'MDU6TGFiZWwxMzQ2OT...  open  \n",
      "22                                                 []  open  \n",
      "23  [{'id': 211029535, 'node_id': 'MDU6TGFiZWwyMTE...  open  \n",
      "24  [{'id': 48070600, 'node_id': 'MDU6TGFiZWw0ODA3...  open  \n",
      "25  [{'id': 134699, 'node_id': 'MDU6TGFiZWwxMzQ2OT...  open  \n",
      "26  [{'id': 48070600, 'node_id': 'MDU6TGFiZWw0ODA3...  open  \n",
      "27  [{'id': 48070600, 'node_id': 'MDU6TGFiZWw0ODA3...  open  \n",
      "28  [{'id': 48070600, 'node_id': 'MDU6TGFiZWw0ODA3...  open  \n",
      "29  [{'id': 48070600, 'node_id': 'MDU6TGFiZWw0ODA3...  open  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://api.github.com/repos/pandas-dev/pandas/issues'\n",
    "resp = requests.get(url)\n",
    "\n",
    "# Response(响应)对象的json方法将返回的一个包含解析为本地Python对象的JSON字典\n",
    "data = resp.json()\n",
    "print(data[0]['title'])\n",
    "\n",
    "issues = pd.DataFrame(data, columns=['number', 'title', 'labels', 'state'])\n",
    "print(issues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.4 与数据库交互\n",
    "在业务场景中，大部分数据并不是存储在文本或Excel文件中。基于SQL的关系型数据库（例如SQL Server、PostgreSQL 和MySQL）使用广泛。很多小众数据库也变得越发流行。数据库的选择通常取决于性能、数据完整性以及应用的可伸缩性需求。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# 使用python内建的sqlite3 驱动来生成一个SQLite 数据库\n",
    "query = \"\"\"\n",
    "CREATE TABLE test\n",
    "(a VARCHAR(20), b VARCHAR(20),\n",
    "c REAL, d INTEGER \n",
    ");\n",
    "\"\"\"\n",
    "con = sqlite3.connect('mydata.sqlite')\n",
    "con.execute(query)\n",
    "con.commit()\n",
    "\n",
    "# 再插入几行数据\n",
    "data = [('Atlanta', 'Georgia', 1.25, 6),\n",
    "       ('Yallahassee', 'Florida', 2.6, 3),\n",
    "       ('Sacramento', 'California', 1.7, 5)]\n",
    "stmt = \"INSERT INTO test VALUES(?, ?, ?, ?)\"\n",
    "con.executemany(stmt, data)\n",
    "con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Atlanta', 'Georgia', 1.25, 6), ('Yallahassee', 'Florida', 2.6, 3), ('Sacramento', 'California', 1.7, 5)]\n",
      "(('a', None, None, None, None, None, None), ('b', None, None, None, None, None, None), ('c', None, None, None, None, None, None), ('d', None, None, None, None, None, None))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Atlanta</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>1.25</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yallahassee</td>\n",
       "      <td>Florida</td>\n",
       "      <td>2.60</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sacramento</td>\n",
       "      <td>California</td>\n",
       "      <td>1.70</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             a           b     c  d\n",
       "0      Atlanta     Georgia  1.25  6\n",
       "1  Yallahassee     Florida  2.60  3\n",
       "2   Sacramento  California  1.70  5"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 从数据库表中选择数据，大部分Python的SQL驱动（PyODBC、psycopg2、MySQLdb、pymssql等）返回的是元祖列表\n",
    "cursor = con.execute('select * from test')\n",
    "rows = cursor.fetchall()\n",
    "print(rows)\n",
    "\n",
    "# 可以将远足的列表传给DataFrame 构造函数，但需要包含在游标的description属性中的列名\n",
    "print(cursor.description)\n",
    "pd.DataFrame(rows, columns=[x[0] for x in cursor.description])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于内容较多，你肯定不想每次查询数据库都重复同样多的步骤。SQLAlchemy 项目是一个流行的Python SQL工具包，抽象去除了SQL数据库之间的许多常见差异。pandas有一个read_sql函数允许你从通过SQLAlchemy 连接中轻松地读取数据。这里，我将使用SQLAlechemy连接到相同的SQLite数据库，并从之前创建的表中读取数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Atlanta</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>1.25</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yallahassee</td>\n",
       "      <td>Florida</td>\n",
       "      <td>2.60</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sacramento</td>\n",
       "      <td>California</td>\n",
       "      <td>1.70</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             a           b     c  d\n",
       "0      Atlanta     Georgia  1.25  6\n",
       "1  Yallahassee     Florida  2.60  3\n",
       "2   Sacramento  California  1.70  5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sqlalchemy as sqla\n",
    "\n",
    "db = sqla.create_engine('sqlite:///mydata.sqlite')\n",
    "pd.read_sql('select * from test', db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
